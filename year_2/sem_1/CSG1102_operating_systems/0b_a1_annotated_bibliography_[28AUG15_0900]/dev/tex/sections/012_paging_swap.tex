\newpage

\subsection{Paging / swap}

\subsubsection*{Alanko, T. O., \& Verkamo, A. I. (1983). Segmentation, paging and optimal page sizes in virtual memory. Performance Evaluation, 3(1), 13–33. doi:10.1016/0167-7136(83)90150-6}

\citetapos{Alanko1983} paper compares the performance between segmentation and paging, and investigates the optimal page size. They introduce the subject by identifying the differences in use, and characteristics of segmentation and paging. The authors claim, that at the time, no studies have yet been published which compare the two memory management techniques. They also identify memory systems which combine segmentation and paging techniques.

\citet{Alanko1983} outline the page size and how the parameter interacts with the performance of paged systems. They consider current studies in the field and come to the conclusion that no consensus has been found in optimum page size proposals.

The paper continues with the experiments, executing programs in segmented and paged environments using the working set policy. \citetapos{Alanko1983} results indicate that ``there is no globally optimal page size''. When using ``space-time integral'' as the key value for measuring performance, segmentation outperforms paging, and the performance gap increases as the mean segment size increases. 

\subsubsection*{Babaoglu, Ö., \& Joy, W. (1981). Converting a swap-based system to do paging in an architecture lacking page-referenced bits. In SOSP ’81 Proceedings of the eighth ACM symposium on Operating systems principles (Vol. 15, pp. 78–86). New York. doi:10.1145/800216.806595}

\citet{Babaoglu1981} describe the challenges and their methods converting UNIX from a segmented, swap-based system to a paging system, in a computer with architecture that does not support page-referenced bits. The authors explain the use of page-referenced bits as essential to page replacement algorithms such as clock page replacement and sampled working set (SWS). They compare various page replacement algorithms, and set out to implement their variation of clock page replacement in UNIX by simulating page-referenced bits through software.

\citet{Babaoglu1981} justify their design and optimization decisions by comparing the performance of the clock page replacement, and identify opportunities for improvement, based on the given hardware and operating system. For instance, \citet[p. 80]{Babaoglu1981} state the original algorithm only seeks to replace a single page when triggered by a page fault, and through testing, found examples where page requests spiked due to UNIX's non-uniform operations. They modified the algorithm by implementing a free page pool containing page frames not currently in the clock loop, and set a minimum free page pool size as a threshold. When this threshold is reached, clock page replacement is triggered and pages are replaced until the free page pool size reaches the threshold again. As the free page pool size decreases, the scan rate of the clock page replacement implementation increases until it reaches a maximum scan rate, which is ``determined by the time it takes to simulate the setting of a referenced bit'' \citep[p. 80]{Babaoglu1981}.

The authors compared their clock paging system to the original swap-based system and present their findings with graphs comparing performance between the two. They found that under lower load levels, their clock page implementation out performed the swap-based method. However, under heavy load, while clock page replacement exhibited much lower page traffic, the overhead required to support it was higher than the swap-based method. The authors counter this finding by stating that the CPU utilization was greater for the clock paging system compared to the swap-based system.

\citet{Babaoglu1981} conclude the article with a recommended requirement when designing a page replacement algorithm for architecture with no support for page-referenced bits, the results of their comparison of the two memory management algorithms, and limitations of their global clock replacement algorithm.

\subsubsection*{Denning, P. J. (1967). The Working Set Model for Program Behavior. In SOSP ’67 Proceedings of the first ACM symposium on Operating System Principles (pp. 15.1–15.12). New York: ACM.}

Although the working set model is neither a virtual memory manager or paging algorithm itself, it is essential to the efficiency of virtual memory \citep{Silberschatz2013}. \citetapos{Denning1967} proposal is a landmark article in the field of computer science. According to google scholar, Denning's paper has been cited 1076 times.

This source introduces the concept of the working set model, which provides the ability for a system to determine what information is being used, or not being used by a program during execution. This ability allows a computer to make better decisions in allocating or deallocating resources to that program.

At the time, the user and the compiler were commonly proposed to be used as input for dynamic memory allocation. \citet[p. 15.1]{Denning1967} claims that neither sources are adequate. He argues that the user cannot possibly provide reliable estimates of resource requirements of his or her program. Additionally, due to the nature of modularised programming, it is possible that the compiler may not be able to decide which modules are required until run time, and therefore not be able to approximate the required resources appropriately.

\citet{Denning1967} explores the current body of work in memory management strategies and states that no paging algorithms have been proposed which involve anticipating resource requirements. The author attributes the cause to the lack of reliable information that the system can use to judge allocation requirements, and returns to the point of the user and compiler as inputs.

\citet[p. 15.2]{Denning1967} proposes ``new mechanisms'' to monitor behaviours of programs, and allocate resources based on those observations. The proposal begins by defining the current environment, briefly explaining the process of paging and what occurs during a page fault. He defines the ``core memory management'' problem as deciding which pages remain resident in main memory. \citet[p. 15.3]{Denning1967} offers a strategy - to ``minimize page traffic'' to reduce overhead required during page swaps, and reduce processing time, which affects processor efficiency if processing pages takes too long. Thus, \citet[p. 15.3]{Denning1967} defines the working set as ``the minimum collection of pages that must be loaded in main memory for a process to operate efficiently, without `unnecessary' page faults''. A more explicit definition is provided as ``the set of its pages a process has referenced within the last $\tau$ seconds of its execution'' \citep[p. 15.4]{Denning1967}.  

He provides hardware and software implementations of the working set model, and justifies the proposal through its practicality in resource allocation for both processor and memory demand strategies.

\citet{Denning1967} concludes the paper with brief comparisons of current allocation strategies, the ideologies of the working set model and iterates the various parameters and properties defined in the implementation the proposal.

\subsubsection*{Silberschatz, A., Galvin, P. B., \& Gagne, G. (2013). Operating System Concepts Essentials (2nd ed.). Hoboken, NJ: Wiley.}

\citetapos{Silberschatz2013} book provides overview of operating systems, and contains a chapter describing virtual memory (p. 371 - 438). The authors present a brief history of virtual memory management and explain the organisation of the virtual address space.

The chapter explains the concept of paging, while exploring the functionality of demand paging and page faults. The book offers several mathematical equations to calculate demand paging efficiency and provides considerably more detail of page replacement and frame allocation algorithms compared to other books in this annotated bibliography. \citet{Silberschatz2013} defines thrashing and the working set model. The chapter concludes by comparing the implementation of virtual memory between two operating systems, Windows and Solaris.

Similar to \citet{Jacob2008}, this book provides clear and concise explanations and diagrams to visually represent concepts.

\subsubsection*{Van Wezenbeek, A. M., \& Jan Withagen, W. J. (1993). A survey of memory management. Microprocessing and Microprogramming, 36(3), 141–162. doi:10.1016/0165-6074(93)90254-I}

\citetapos{VanWezenbeek1993} paper explores the body of literature on the topic of memory management and virtual memory to provide an overview of the research on the subject. They do not compare existing memory management technologies, however the paper serves to highlight the ``state of the art research'' on the topic \citep[p. 141]{VanWezenbeek1993}.

\citet{VanWezenbeek1993} define the three policies of mechanisms which enable paging and segmentation. First, the fetch policy determines when information will be loaded into main memory, which can either be on demand, in advance, also known as pre-fetching, or both, called demand prefetching. Prefetching relies on the spatial locality of reference. In other words, if a page is referenced, then it is also likely that surrounding pages will also be referenced. The probability of being referenced decreases as the distance from the initially referenced page increases. Secondly, the replacement policy determines which page or segment should be replaced in order to regain required space in main memory. The third policy is the replacement policy, which decides where to place new information in the free space of main memory.

The authors assert that efficiency of virtual memory is dependant on the working set, based on temporal locality of reference, and stress that good replacement policy should preserve the working set of a program. Alternatively, spatial locality, as mentioned previously can also help to achieve virtual memory efficiency, by structuring the program so that its pages are close to each other. They cite sources stating that this can have a greater impact on performance than the choice of a replacement policy, however this falls under the responsibility of the programmer.

\citet{VanWezenbeek1993} outline the functionality of paging and due to its nature of operation, causes internal fragmentation. They explain how it works and the requirements to support paging, which includes the maintenance of various tables. For example, the authors define the information that is included in a page table, and which bits provide protection of memory in a multiprogramming environment.

\citet{VanWezenbeek1993} define the process of segmentation and in contrast, causes external fragmentation. Similar tables to paging are maintained for segmentation, but also needs to keep track of segment length. They stress that a good placement policy is required for segmentation to minimize external fragmentation.

Replacement policies are explained in depth. This section is further subdivided into fixed allocation policies, such as ``First In, First Out (FIFO)'', ``Least Recently Used (LRU)'', ``Most Frequently Used (MFU)'', ``Not Recently Used (NRU)'', ``Clock (CLK)'', and ``Second Chance (SC)''. Variable allocation policies include ``Working Set (WS)'', ``Page Fault Frequency (PFF)'', ``Damped Working Set (DWS)'', ``Sampled Working Set (SWS)'', ``Working Set Clock (WSC)'', and ``Variable Interval Sampled Working Set (VSWS)''.

\citet{VanWezenbeek1993} explore placement policies. They state that this is operation is trivial, and works similarly for both placement and replacement policies in a paging environment. However, the process is more complicated in a segmented environment. As segmentation occurs, free blocks will be separated, and the list of these free blocks will increase as main memory is filled. A suitable block of memory must be searched for in this list when allocating memory. \citet{VanWezenbeek1993} outline the consequences of the order in which the blocks sit within the free space list.